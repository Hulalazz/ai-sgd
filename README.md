# Averaged implicit stochastic gradient descent (AI-SGD)

This is the accompanying code implementation of the methods and algorithms
for a paper in progress.

## References
* Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Regularization paths
  for generalized linear models via coordinate descent. *Journal of Statistical
  Software*, 33(1):1-22, 2010.
* Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using
  predictive variance reduction. *Advances in Neural Information Processing
  Systems*, 2013.
* Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method
  with an exponential convergence rate for finite training sets. *Advances in
  Neural Information Processing Systems*, 2012.
* David Ruppert. Efficient estimations from a slowly convergent robbins-monro
  process. Technical report, Cornell University Operations Research and
  Industrial Engineering, 1988.
* Wei Xu. Towards optimal one pass large scale learning with averaged stochastic
  gradient descent. *arXiv preprint
  [arXiv:1107.2490](http://arxiv.org/abs/1107.2490)*, 2011.
